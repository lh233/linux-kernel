KSM在初始化时会创建一个名为“ksmd”的内核线程。

```
[mm/ksm.c]

static int __init ksm_init(void)
{
	struct task_struct *ksm_thread;
	int err;

	err = ksm_slab_init();
	if (err)
		goto out;

	ksm_thread = kthread_run(ksm_scan_thread, NULL, "ksmd");
	if (IS_ERR(ksm_thread)) {
		pr_err("ksm: creating kthread failed\n");
		err = PTR_ERR(ksm_thread);
		goto out_free;
	}

#ifdef CONFIG_SYSFS
	err = sysfs_create_group(mm_kobj, &ksm_attr_group);
	if (err) {
		pr_err("ksm: register sysfs failed\n");
		kthread_stop(ksm_thread);
		goto out_free;
	}
#else
	ksm_run = KSM_RUN_MERGE;	/* no way for user to start it */

#endif /* CONFIG_SYSFS */

#ifdef CONFIG_MEMORY_HOTREMOVE
	/* There is no significance to this priority 100 */
	hotplug_memory_notifier(ksm_memory_callback, 100);
#endif
	return 0;

out_free:
	ksm_slab_free();
out:
	return err;
}
```

KSM只会处理通过madvise系统调用显式指定的用户进程空间内存，因此用户程序想使用这个功能就必须在分配内存时显式地调用“madvise(addr,length,MADV_MERGEABLE)”，如果用户想在KSM中取消某一个用户进程地址空间的合并功能，也需要显式地用“madvise(addr,length,MADV_UNMERGEABLE)"。

在Android系统中，在libc库（Android系统的libc库是bionic)中的mmap函数实现已经默认添加了此功能。

```
static bool kernel has MADV_MERGEABLE=true;
void* mmap64(void* addr,sizet size,int prot,int flags,int fd,off64_t offset){
	bool is private anonymous=(flags & (MAP_PRIVATE |MAP_ANONYMOUS))!=0; 
	void* result=mmap2(addr,size,prot,flags,fd,offset >>MMAP2_SHIFT); 
	if(result!=MAP FAILED && kernel has MADV MERGEABLE && is_private_anonymous
	{
		int rc =madvise(result,size,MADV_ MERGEABLE); 
		if (rc==-1 && errno==EINVAL){ 
			kernel_has_MADV_MERGEABLE=false;
		}
	}
    return result;
}
void* mmap(void* addr,size t size,int prot,int flags,int fd,off_t offset){
	return mmap64(addr,size,prot,flags,fd,static_cast<off64_t>((unsigned 1ong)offset));
}
    
```

第5-11行，判断mmap分配的内存，即进程用户空间地址是否私有映射（MAP_PRIVATE）或者匿名映射（MAP_ANONYMOUS），如果是，则显式调用madivese系统把进程用户空间地址区间添加到Linux内核KSM系统中。

```
int __ksm_enter(struct mm_struct *mm)
{
	struct mm_slot *mm_slot;
	int needs_wakeup;

	mm_slot = alloc_mm_slot();
	if (!mm_slot)
		return -ENOMEM;

	/* Check ksm_run too?  Would need tighter locking */
	needs_wakeup = list_empty(&ksm_mm_head.mm_list);

	spin_lock(&ksm_mmlist_lock);
	insert_to_mm_slots_hash(mm, mm_slot);
	/*
	 * When KSM_RUN_MERGE (or KSM_RUN_STOP),
	 * insert just behind the scanning cursor, to let the area settle
	 * down a little; when fork is followed by immediate exec, we don't
	 * want ksmd to waste time setting up and tearing down an rmap_list.
	 *
	 * But when KSM_RUN_UNMERGE, it's important to insert ahead of its
	 * scanning cursor, otherwise KSM pages in newly forked mms will be
	 * missed: then we might as well insert at the end of the list.
	 */
	if (ksm_run & KSM_RUN_UNMERGE)
		list_add_tail(&mm_slot->mm_list, &ksm_mm_head.mm_list);
	else
		list_add_tail(&mm_slot->mm_list, &ksm_scan.mm_slot->mm_list);
	spin_unlock(&ksm_mmlist_lock);

	set_bit(MMF_VM_MERGEABLE, &mm->flags);
	atomic_inc(&mm->mm_count);

	if (needs_wakeup)
		wake_up_interruptible(&ksm_thread_wait);

	return 0;
}
```

第6行代码,分配一个 struct mm_slot数据结构。

第13行代码,添加管理 ksm_mmlist链表的 spinlock锁

第14行代码,把当前的mm数据结构添加到mm_slots_hash哈希表中。

第25-28行代码,把 mm slot添加到 ksm_scan_mm slot-> mm_list链表中。

第31行代码,设置mm->fags中的 MMF_VM_MERGEABLE标志位,表示这个进程已经添加到KSM系统中。

第34~35行代码,如果之前 ksm_mm_head.mm_list链表为空,则唤醒ksmd内核线程。

```
[ksmd内核线程]
static int ksm_scan_thread(void *nothing)
{
	set_freezable();
	set_user_nice(current, 5);

	while (!kthread_should_stop()) {
		mutex_lock(&ksm_thread_mutex);
		wait_while_offlining();
		if (ksmd_should_run())
			ksm_do_scan(ksm_thread_pages_to_scan);
		mutex_unlock(&ksm_thread_mutex);

		try_to_freeze();

		if (ksmd_should_run()) {
			schedule_timeout_interruptible(
				msecs_to_jiffies(ksm_thread_sleep_millisecs));
		} else {
			wait_event_freezable(ksm_thread_wait,
				ksmd_should_run() || kthread_should_stop());
		}
	}
	return 0;
}
```

ksm_scan_thread是ksmd内核线程的主干,每次会执行 ksm_do_scan()函数去扫描和合并100个页面(见 ksm_thread_pages_to_scan变量),然后睡眠等待20毫秒(见 ksm_thread_sleepmillisecs变量),这两个参数可以在“/ sys/kernel/mm/ksm”目录下的相关参数中去设置和修改。

```
[ksmd内核线程]
static void ksm_do_scan(unsigned int scan_npages)
{
	struct rmap_item *rmap_item;
	struct page *uninitialized_var(page);

	while (scan_npages-- && likely(!freezing(current))) {
		cond_resched();
		rmap_item = scan_get_next_rmap_item(&page);
		if (!rmap_item)
			return;
		cmp_and_merge_page(page, rmap_item);
		put_page(page);
	}
}
```

ksm_do_scan()函数在 while循环中尝试去合并 scan_npages个页面, scan_get_next_rmap_item()获取一个合适的匿名页面page, cmp_and_merge_page()会让page 在在KSM中的stable和unstable 两棵红黑树中查找是否有合适合并的对象,并且尝试去合并它们。下面首先来看KSM的核心数据结构。

```
[mm/ksm.c]
struct rmap_item {
	struct rmap_item *rmap_list;
	union {
		struct anon_vma *anon_vma;	/* when stable */
#ifdef CONFIG_NUMA
		int nid;		/* when node of unstable tree */
#endif
	};
	struct mm_struct *mm;
	unsigned long address;		/* + low bits used for flags below */
	unsigned int oldchecksum;	/* when unstable */
	union {
		struct rb_node node;	/* when node of unstable tree */
		struct {		/* when listed from stable tree */
			struct stable_node *head;
			struct hlist_node hlist;
		};
	};
};

struct mm_slot {
	struct hlist_node link;
	struct list_head mm_list;
	struct rmap_item *rmap_list;
	struct mm_struct *mm;
};

struct ksm_scan {
	struct mm_slot *mm_slot;
	unsigned long address;
	struct rmap_item **rmap_list;
	unsigned long seqnr;
};
```

rmap_item 数据结构描述一个虚拟地址反向映射的条目(item)。

- rmap_list:所有的rmap_item连接成一个链表,链表头在ksm_scan.rmap_list中。
- anon_vma:当rmap_item加入stable树时，指向VMA的anon_vma数据结构。
- mm:进程的struct mm_struct数据结构。
- address: rmap_item所跟踪的用户空间地址，
- oldchecksum:虚拟地址对应的物理页面的旧校验值。
- node: rmap_item加入unstable红黑树的节点。
- head:加入stable红黑树的节点。
- hlist: stable链表。

mmslot数据结构描述添加到KSM系统中将要被扫描的进程mm struct数据结构。

- link:用于添加到mm_slot哈希表中。
- mm_list:用于添加到mm_slot链表中，银链表头在ksm_mm_head。
- rmap_list: rmap_item链表头。
- mm:进程的mm数据结构。

ksm_scan数据结构用于表示当前扫描的状态。

- mm_slot:当前正在扫描的mm_slot。
- address:下一次扫描地址。
- rmap_list:将要扫描rmap_item的指针。
- seqnr:全部扫描完成后会计数一次，用于删除 unstable节点。

```
[mm/ksm.c]
static struct mm_slot ksm_mm_head = {
	.mm_list = LIST_HEAD_INIT(ksm_mm_head.mm_list),
};
static struct ksm_scan ksm_scan = {
	.mm_slot = &ksm_mm_head,
};
```

ksm_mm_head是mm_slot链表的头。ksm_scan是静态全局的数据结构，用于描述当前扫描的 mm_slot.

下面来看ksm_do_scan()中scan_get_next_map_item()函数的实现。

```
[ksm_do_scan()->scan_get_next_rmap_item()]
static struct rmap_item *scan_get_next_rmap_item(struct page **page)
{
	struct mm_struct *mm;
	struct mm_slot *slot;
	struct vm_area_struct *vma;
	struct rmap_item *rmap_item;
	int nid;

	if (list_empty(&ksm_mm_head.mm_list))
		return NULL;

	slot = ksm_scan.mm_slot;
	if (slot == &ksm_mm_head) {
		/*
		 * A number of pages can hang around indefinitely on per-cpu
		 * pagevecs, raised page count preventing write_protect_page
		 * from merging them.  Though it doesn't really matter much,
		 * it is puzzling to see some stuck in pages_volatile until
		 * other activity jostles them out, and they also prevented
		 * LTP's KSM test from succeeding deterministically; so drain
		 * them here (here rather than on entry to ksm_do_scan(),
		 * so we don't IPI too often when pages_to_scan is set low).
		 */
		lru_add_drain_all();

		/*
		 * Whereas stale stable_nodes on the stable_tree itself
		 * get pruned in the regular course of stable_tree_search(),
		 * those moved out to the migrate_nodes list can accumulate:
		 * so prune them once before each full scan.
		 */
		if (!ksm_merge_across_nodes) {
			struct stable_node *stable_node;
			struct list_head *this, *next;
			struct page *page;

			list_for_each_safe(this, next, &migrate_nodes) {
				stable_node = list_entry(this,
						struct stable_node, list);
				page = get_ksm_page(stable_node, false);
				if (page)
					put_page(page);
				cond_resched();
			}
		}

		for (nid = 0; nid < ksm_nr_node_ids; nid++)
			root_unstable_tree[nid] = RB_ROOT;

		spin_lock(&ksm_mmlist_lock);
		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
		ksm_scan.mm_slot = slot;
		spin_unlock(&ksm_mmlist_lock);
		/*
		 * Although we tested list_empty() above, a racing __ksm_exit
		 * of the last mm on the list may have removed it since then.
		 */
		if (slot == &ksm_mm_head)
			return NULL;
next_mm:
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
	}

	mm = slot->mm;
	down_read(&mm->mmap_sem);
	if (ksm_test_exit(mm))
		vma = NULL;
	else
		vma = find_vma(mm, ksm_scan.address);

	for (; vma; vma = vma->vm_next) {
		if (!(vma->vm_flags & VM_MERGEABLE))
			continue;
		if (ksm_scan.address < vma->vm_start)
			ksm_scan.address = vma->vm_start;
		if (!vma->anon_vma)
			ksm_scan.address = vma->vm_end;

		while (ksm_scan.address < vma->vm_end) {
			if (ksm_test_exit(mm))
				break;
			*page = follow_page(vma, ksm_scan.address, FOLL_GET);
			if (IS_ERR_OR_NULL(*page)) {
				ksm_scan.address += PAGE_SIZE;
				cond_resched();
				continue;
			}
			if (PageAnon(*page) ||
			    page_trans_compound_anon(*page)) {
				flush_anon_page(vma, *page, ksm_scan.address);
				flush_dcache_page(*page);
				rmap_item = get_next_rmap_item(slot,
					ksm_scan.rmap_list, ksm_scan.address);
				if (rmap_item) {
					ksm_scan.rmap_list =
							&rmap_item->rmap_list;
					ksm_scan.address += PAGE_SIZE;
				} else
					put_page(*page);
				up_read(&mm->mmap_sem);
				return rmap_item;
			}
			put_page(*page);
			ksm_scan.address += PAGE_SIZE;
			cond_resched();
		}
	}

	if (ksm_test_exit(mm)) {
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
	}
	/*
	 * Nuke all the rmap_items that are above this current rmap:
	 * because there were no VM_MERGEABLE vmas with such addresses.
	 */
	remove_trailing_rmap_items(slot, ksm_scan.rmap_list);

	spin_lock(&ksm_mmlist_lock);
	ksm_scan.mm_slot = list_entry(slot->mm_list.next,
						struct mm_slot, mm_list);
	if (ksm_scan.address == 0) {
		/*
		 * We've completed a full scan of all vmas, holding mmap_sem
		 * throughout, and found no VM_MERGEABLE: so do the same as
		 * __ksm_exit does to remove this mm from all our lists now.
		 * This applies either when cleaning up after __ksm_exit
		 * (but beware: we can reach here even before __ksm_exit),
		 * or when all VM_MERGEABLE areas have been unmapped (and
		 * mmap_sem then protects against race with MADV_MERGEABLE).
		 */
		hash_del(&slot->link);
		list_del(&slot->mm_list);
		spin_unlock(&ksm_mmlist_lock);

		free_mm_slot(slot);
		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
		up_read(&mm->mmap_sem);
		mmdrop(mm);
	} else {
		spin_unlock(&ksm_mmlist_lock);
		up_read(&mm->mmap_sem);
	}

	/* Repeat until we've completed scanning the whole list */
	slot = ksm_scan.mm_slot;
	if (slot != &ksm_mm_head)
		goto next_mm;

	ksm_scan.seqnr++;
	return NULL;
}
```

第10行代码, ksmmm_head链表为空,则不进行扫描。

第14~64 行代码，ksmd第一次跑的情况，初始化ksm_scan  数据结构中的成员ksm_scan.mm_slot、ksm_scan.address和ksm_scan.rmap_list。

第66~107行代码，扫描当前slot 对应的用户进程中的所有VMAs,寻寻找一个合适的匿名页面。

第73行代码，for循环遍历所有VMA。

第81~107行代码，扫描VMA中所有的虚拟页面，follow_page()函数从虚拟地址开始找回normal mapping页面的struct page数据结构，KSM只会处理匿名页面的情况

第90行代码,使用PageAnon()来判断该,是否为匿名页面。

第92~93行代码，，冲刷该页对应的cache。get_next_rmap_item()去找mm_slot->rmap_list链表上是否有该虚拟地址对应的rmap_item，没找到就新建一个。

第97行代码, ksm_scan.rmap_list指向刚找到或者新建的rmap_item,方便后续的扫描。找到合适的匿名页面后,释放mm->mmapsem信号量,这个信号量是在扫描VMA时加的,然后返回该页struct page数据结构。

第111行代码，运行到这里说明for循环里扫描该进程所有的VMA都没找到合适的匿名页面，，因为如果找到一个合适的匿名页面是会返回rmap_item的。如果被扫描的进程已经被销毁了(mm->mm_users=0)，那么设置ksm_scan.address=0，第124~141行代码会处理这个情况。

第122行代码，在该进程中没找到合适的匿名页面时，那么对应的rmap_item已经没有用处为了避免占用内存空间，直接全部删掉，

第122行代码，取下一个mm_slot, 这里操作了 mm_slot链表，所以用一个spinlock锁ksm_mmlist_lock来保护链表.

第124~141行代码，处理该进程被销毁的情况，把mm_slot从ksm_mm_ head链表删除，释放mm_slot数据结构，清空mm->flags中的MMF_VM_MERGEABLE标志位。

第148~150行代码,如果没有扫描完一轮所有的 mm_slot,那就继续扫描下一个 mm_slot

第152行代码,如果扫描完一轮 mm_slot,则增加 ksm_scan.seqnr计数。

下面回到ksm_do_scan()函数中的cmp_and_merge_page()函数。

```
static void cmp_and_merge_page(struct page *page, struct rmap_item *rmap_item)
{
	struct rmap_item *tree_rmap_item;
	struct page *tree_page = NULL;
	struct stable_node *stable_node;
	struct page *kpage;
	unsigned int checksum;
	int err;

	stable_node = page_stable_node(page);
	if (stable_node) {
		if (stable_node->head != &migrate_nodes &&
		    get_kpfn_nid(stable_node->kpfn) != NUMA(stable_node->nid)) {
			rb_erase(&stable_node->node,
				 root_stable_tree + NUMA(stable_node->nid));
			stable_node->head = &migrate_nodes;
			list_add(&stable_node->list, stable_node->head);
		}
		if (stable_node->head != &migrate_nodes &&
		    rmap_item->head == stable_node)
			return;
	}

	/* We first start with searching the page inside the stable tree */
	kpage = stable_tree_search(page);
	if (kpage == page && rmap_item->head == stable_node) {
		put_page(kpage);
		return;
	}

	remove_rmap_item_from_tree(rmap_item);

	if (kpage) {
		err = try_to_merge_with_ksm_page(rmap_item, page, kpage);
		if (!err) {
			/*
			 * The page was successfully merged:
			 * add its rmap_item to the stable tree.
			 */
			lock_page(kpage);
			stable_tree_append(rmap_item, page_stable_node(kpage));
			unlock_page(kpage);
		}
		put_page(kpage);
		return;
	}

	/*
	 * If the hash value of the page has changed from the last time
	 * we calculated it, this page is changing frequently: therefore we
	 * don't want to insert it in the unstable tree, and we don't want
	 * to waste our time searching for something identical to it there.
	 */
	checksum = calc_checksum(page);
	if (rmap_item->oldchecksum != checksum) {
		rmap_item->oldchecksum = checksum;
		return;
	}

	tree_rmap_item =
		unstable_tree_search_insert(rmap_item, page, &tree_page);
	if (tree_rmap_item) {
		kpage = try_to_merge_two_pages(rmap_item, page,
						tree_rmap_item, tree_page);
		put_page(tree_page);
		if (kpage) {
			/*
			 * The pages were successfully merged: insert new
			 * node in the stable tree and add both rmap_items.
			 */
			lock_page(kpage);
			stable_node = stable_tree_insert(kpage);
			if (stable_node) {
				stable_tree_append(tree_rmap_item, stable_node);
				stable_tree_append(rmap_item, stable_node);
			}
			unlock_page(kpage);

			/*
			 * If we fail to insert the page into the stable tree,
			 * we will have 2 virtual addresses that are pointing
			 * to a ksm page left outside the stable tree,
			 * in which case we need to break_cow on both.
			 */
			if (!stable_node) {
				break_cow(tree_rmap_item);
				break_cow(rmap_item);
			}
		}
	}
}
```

cmp_and_merge_page()函数有两个参数, page表示刚才扫描mmslot时找到的一个合格的匿名页面, rmap_item表示该page对应的rmap_item数据结构。

第10行代码，如果这个页面是stable_node，否则page_stable_node()返回这个page对应的stable_node，否则返回NULL。

第25行代码，
